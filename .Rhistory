install.packages("bookdown")
install.packages("bookdown")
library(bookdown)
install.packages("downlit")
library(downlit)
library(downlit)
knitr::include_graphics("images/tmwr_0201.png")
library(tidytext)
load("data/afinn.rda")
afinn
get_sentiments("bing")
load("data/nrc.rda")
nrc
library(janeaustenr)
library(dplyr)
library(stringr)
tidy_books <- austen_books() %>%
group_by(book) %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
nrc_joy <- nrc %>%
filter(sentiment == "joy")
tidy_books %>%
filter(book == "Emma") %>%
inner_join(nrc_joy) %>%
count(word, sort = TRUE)
library(tidyr)
jane_austen_sentiment <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(book, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(sentiment = positive - negative)
library(ggplot2)
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
geom_col(show.legend = FALSE) +
facet_wrap(~book, ncol = 2, scales = "free_x")
pride_prejudice <- tidy_books %>%
filter(book == "Pride & Prejudice")
pride_prejudice
afinn <- pride_prejudice %>%
inner_join(afinn) %>%
group_by(index = linenumber %/% 80) %>%
summarise(sentiment = sum(value)) %>%
mutate(method = "AFINN")
bing_and_nrc <- bind_rows(
pride_prejudice %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing et al."),
pride_prejudice %>%
inner_join(nrc %>%
filter(sentiment %in% c("positive",
"negative"))
) %>%
mutate(method = "NRC")) %>%
count(method, index = linenumber %/% 80, sentiment) %>%
pivot_wider(names_from = sentiment,
values_from = n,
values_fill = 0) %>%
mutate(sentiment = positive - negative)
bind_rows(afinn,
bing_and_nrc) %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = FALSE) +
facet_wrap(~method, ncol = 1, scales = "free_y")
nrc %>%
filter(sentiment %in% c("positive", "negative")) %>%
count(sentiment)
get_sentiments("bing") %>%
count(sentiment)
bing_word_counts <- tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
ungroup()
bing_word_counts
bing_word_counts %>%
group_by(sentiment) %>%
slice_max(n, n = 10) %>%
ungroup() %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, fill = sentiment)) +
geom_col(show.legend = FALSE) +
facet_wrap(~sentiment, scales = "free_y") +
labs(x = "Contribution to sentiment",
y = NULL)
custom_stop_words <- bind_rows(tibble(word = c("miss"),
lexicon = c("custom")),
stop_words)
custom_stop_words
library(wordcloud)
tidy_books %>%
anti_join(stop_words) %>%
count(word) %>%
with(wordcloud(word, n, max.words = 100))
library(reshape2)
tidy_books %>%
inner_join(get_sentiments("bing")) %>%
count(word, sentiment, sort = TRUE) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("gray20", "gray80"),
max.words = 100)
p_and_p_sentences <- tibble(text = prideprejudice) %>%
unnest_tokens(sentence, text, token = "sentences")
p_and_p_sentences$sentence[2]
austen_chapters <- austen_books() %>%
group_by(book) %>%
unnest_tokens(chapter, text, token = "regex",
pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
ungroup()
austen_chapters %>%
group_by(book) %>%
summarise(chapters = n())
bingnegative <- get_sentiments("bing") %>%
filter(sentiment == "negative")
wordcounts <- tidy_books %>%
group_by(book, chapter) %>%
summarize(words = n())
tidy_books %>%
semi_join(bingnegative) %>%
group_by(book, chapter) %>%
summarize(negativewords = n()) %>%
left_join(wordcounts, by = c("book", "chapter")) %>%
mutate(ratio = negativewords/words) %>%
filter(chapter != 0) %>%
slice_max(ratio, n = 1) %>%
ungroup()
View(afinn)
View(nrc)
View(tidy_books)
gmf_data <- read_lines("https://raw.githubusercontent.com/hbedros/data607_hw10/main/gmf.txt")
library(tidyverse)
library(tidytext)
gmf_data <- read_lines("https://raw.githubusercontent.com/hbedros/data607_hw10/main/gmf.txt")
gmf_data_tibble <- tibble(text = gmf_data)
View(gmf_data_tibble)
tidy_gmf <- gmf_data_tibble %>%
group_by(book = "gmf") %>%
mutate(
linenumber = row_number(),
chapter = cumsum(str_detect(text,
regex("^chapter [\\divxlc]",
ignore_case = TRUE)))) %>%
ungroup() %>%
unnest_tokens(word, text)
View(tidy_gmf)
sentiwordnet <- read.delim("data/SentiWordNet_3.0.0.txt", header=TRUE)
View(sentiwordnet)
head(sentiwordnet)
sentiwordnet <- read.delim("data/SentiWordNet_3.0.0.txt", header=TRUE, skip=6)
head(sentiwordnet)
sentiwordnet <- read.delim("data/SentiWordNet_3.0.0.txt", header=TRUE)
View(sentiwordnet)
head(sentiwordnet)
library(tidyverse)
library(tokenizers)
# Load the corpus
corpus_url <- "https://raw.githubusercontent.com/hbedros/data607_hw10/main/gmf.txt"
corpus <- read_lines(corpus_url)
# Load SentiWordNet data (assuming you've already loaded it into an object called 'sentiwordnet')
sentiwordnet <- read.delim("data/SentiWordNet_3.0.0.txt", header=TRUE)
tokens <- unlist(tokenize_words(corpus))
matched_tokens <- sentiwordnet %>%
unnest(SynsetTerms = str_split(SynsetTerms, " ")) %>%
filter(SynsetTerms %in% tokens)
sentiment_scores <- matched_tokens %>%
group_by(SynsetTerms) %>%
summarise(
AveragePosScore = mean(as.numeric(PosScore), na.rm = TRUE),
AverageNegScore = mean(as.numeric(NegScore), na.rm = TRUE)
) %>%
mutate(NetSentiment = AveragePosScore - AverageNegScore)
top_words <- sentiment_scores %>%
arrange(desc(NetSentiment)) %>%
head(10)
bottom_words <- sentiment_scores %>%
arrange(NetSentiment) %>%
head(10)
# Plot
ggplot(rbind(top_words, bottom_words), aes(x = reorder(SynsetTerms, NetSentiment), y = NetSentiment, fill = NetSentiment > 0)) +
geom_col() +
coord_flip() +
labs(title = "Top Positive and Negative Words based on Net Sentiment Score", x = "Words", y = "Net Sentiment Score") +
scale_fill_manual(values = c("red", "green"), name = "Sentiment", labels = c("Negative", "Positive"))
View(sentiment_scores)
corpus_url <- "https://raw.githubusercontent.com/hbedros/data607_hw10/main/gmf.txt"
corpus <- read_lines(corpus_url)
View(wordcounts)
View(sentiwordnet)
tokens <- unlist(tokenize_words(corpus))
matched_tokens <- sentiwordnet %>%
unnest(SynsetTerms = str_split(SynsetTerms, " ")) %>%
filter(SynsetTerms %in% tokens)
matched_tokens <- sentiwordnet %>%
mutate(SynsetTerms = str_split(SynsetTerms, " ")) %>%
unnest(c(SynsetTerms)) %>%
filter(SynsetTerms %in% tokens)
View(matched_tokens)
View(matched_tokens)
View(matched_tokens)
View(matched_tokens)
View(matched_tokens)
sentiment_scores <- matched_tokens %>%
group_by(SynsetTerms) %>%
summarise(
AveragePosScore = mean(as.numeric(PosScore), na.rm = TRUE),
AverageNegScore = mean(as.numeric(NegScore), na.rm = TRUE)
) %>%
mutate(NetSentiment = AveragePosScore - AverageNegScore)
+   unnest(SynsetTerms = str_split(SynsetTerms, " ")) %>%
head(matched_tokens)
head(tokens)
# Inspecting the first few SynsetTerms
head(unique(sentiwordnet$SynsetTerms))
View(sentiwordnet)
View(matched_tokens)
View(nrc)
View(sentiwordnet)
View(sentiment_scores)
View(sentiwordnet)
View(sentiment_scores)
View(sentiwordnet)
corpus_url <- "https://raw.githubusercontent.com/hbedros/data607_hw10/main/gmf.txt"
corpus <- read_lines(corpus_url)
sentiwordnet <- read.delim("data/SentiWordNet_3.0.0.txt", header=TRUE)
View(sentiwordnet)
tokens <- unlist(tokenize_words(corpus))
View(nrc)
View(matched_tokens)
View(matched_tokens)
View(matched_tokens)
View(matched_tokens)
View(sentiwordnet)
View(sentiwordnet)
head(sentiwordnet)
library(dplyr)
library(tidyr)
library(stringr)
sentiwordnet <- read.delim("data/SentiWordNet_3.0.0.txt", header=TRUE)
cleaned_sentiwordnet <- sentiwordnet %>%
select(PosScore, NegScore, SynsetTerms) %>%               # Retain the needed columns
separate_rows(SynsetTerms, sep = " ") %>%                # Split SynsetTerms on spaces
mutate(SynsetTerms = str_remove(SynsetTerms, "#\\d+$"))  # Remove # and numbers
head(cleaned_sentiwordnet)
View(cleaned_sentiwordnet)
tokens <- unlist(tokenize_words(corpus))
matched_tokens <- cleaned_sentiwordnet %>%
mutate(SynsetTerms = str_split(SynsetTerms, " ")) %>%
unnest(c(SynsetTerms)) %>%
filter(SynsetTerms %in% tokens)
View(matched_tokens)
View(matched_tokens)
sentiment_scores <- matched_tokens %>%
group_by(SynsetTerms) %>%
summarise(
AveragePosScore = mean(as.numeric(PosScore), na.rm = TRUE),
AverageNegScore = mean(as.numeric(NegScore), na.rm = TRUE)
) %>%
mutate(NetSentiment = AveragePosScore - AverageNegScore)
View(sentiment_scores)
top_words <- sentiment_scores %>%
arrange(desc(NetSentiment)) %>%
head(10)
bottom_words <- sentiment_scores %>%
arrange(NetSentiment) %>%
head(10)
# Plot
ggplot(rbind(top_words, bottom_words), aes(x = reorder(SynsetTerms, NetSentiment), y = NetSentiment, fill = NetSentiment > 0)) +
geom_col() +
coord_flip() +
labs(title = "Top Positive and Negative Words based on Net Sentiment Score", x = "Words", y = "Net Sentiment Score") +
scale_fill_manual(values = c("red", "green"), name = "Sentiment", labels = c("Negative", "Positive"))
subj_lines <- readLines("data/subjclueslen1-HLTEMNLP05.tff")
parsed_data <- lapply(subj_lines, function(line) {
pairs <- strsplit(line, " ")[[1]]
sapply(pairs, function(pair) {
strsplit(pair, "=")[[1]][2]
})
})
View(parsed_data)
subj_clues <- do.call(rbind.data.frame, parsed_data)
names(subj_clues) <- sapply(strsplit(subj_lines[1], " "), function(x) strsplit(x, "=")[[1]][1])
View(subj_clues)
head(sub_clues)
head(subj_clues)
subj_clues <- subj_clues[, c(1, 2, 3, 4, 5)]
colnames(subj_clues) <- c("type", "length", "word", "POS", "priorpolarity")
View(subj_clues)
subj_lines <- readLines("data/subjclueslen1-HLTEMNLP05.tff")
# Parse the key-value pairs
parsed_data <- lapply(subj_lines, function(line) {
pairs <- strsplit(line, " ")[[1]]
sapply(pairs, function(pair) {
strsplit(pair, "=")[[1]][2]
})
})
# Convert list to dataframe
subj_clues <- do.call(rbind.data.frame, parsed_data)
names(subj_clues) <- sapply(strsplit(subj_lines[1], " "), function(x) strsplit(x, "=")[[1]][1])
subj_clues <- subj_clues[, c(1, 2, 3, 4, 6)]
colnames(subj_clues) <- c("type", "length", "word", "POS", "priorpolarity")
View(subj_clues)
matched_tokens <- subj_clues %>%
filter(word %in% tokens)
View(matched_tokens)
matched_tokens2 <- subj_clues %>%
filter(word %in% tokens)
sentiment_scores <- matched_tokens2 %>%
mutate(score = case_when(
priorpolarity == "positive" ~ 1,
priorpolarity == "negative" ~ -1,
TRUE ~ 0
))
overall_sentiment_score <- sum(sentiment_scores$score)
sentiment_scores <- matched_tokens2 %>%
mutate(score = case_when(
priorpolarity == "positive" ~ 1,
priorpolarity == "negative" ~ -1,
TRUE ~ 0
))
overall_sentiment_score <- sum(sentiment_scores$score)
sentiment_scores <- matched_tokens2 %>%
mutate(score = case_when(
priorpolarity == "positive" ~ 1,
priorpolarity == "negative" ~ -1,
TRUE ~ 0
))
overall_sentiment_score <- sum(sentiment_scores$score)
ggplot(sentiment_labels, aes(x = label)) +
geom_bar(fill = c("red", "blue", "gray")) +
labs(title = "Sentiment Distribution in the Corpus",
x = "Sentiment",
y = "Count") +
theme_minimal()
ggplot(sentiment_labels, aes(x = label)) +
geom_bar(fill = c("red", "blue", "gray")) +
labs(title = "Sentiment Distribution in the Corpus",
x = "Sentiment",
y = "Count") +
theme_minimal()
overall_sentiment_score <- sum(sentiment_scores$score)
paste("The overall sentiment score of the corpus is:", overall_sentiment_score)
matched_tokens2$label <- case_when(
matched_tokens2$priorpolarity == "positive" ~ "Positive",
matched_tokens2$priorpolarity == "negative" ~ "Negative",
TRUE ~ "Neutral"
)
# Plot using ggplot2
plot <- ggplot(matched_tokens2, aes(x = label)) +
geom_bar(fill = c("green", "red", "gray"), width = 0.5) +
labs(title = "Sentiment Distribution in the Corpus",
x = "Sentiment",
y = "Count of Words") +
theme_minimal()
print(plot)
matched_tokens2$label <- case_when(
matched_tokens2$priorpolarity == "positive" ~ "Positive",
matched_tokens2$priorpolarity == "negative" ~ "Negative",
TRUE ~ "Neutral"
)
# Plot using ggplot2
plot <- ggplot(matched_tokens2, aes(x = label)) +
geom_bar(fill = c("green", "red", "gray"), width = 0.5) +
labs(title = "Sentiment Distribution in the Corpus",
x = "Sentiment",
y = "Count of Words") +
theme_minimal()
print(plot)
